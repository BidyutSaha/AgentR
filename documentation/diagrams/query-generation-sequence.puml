@startuml query-generation-sequence

title Stage 2: Query Generation - Sequence Diagram

actor User
participant "Frontend" as FE
participant "API Gateway" as API
participant "Auth Middleware" as Auth
participant "Query Controller" as Controller
participant "Query Service" as Service
participant "OpenAI API" as OpenAI

User -> FE: Submit Stage 1 output
activate FE
note right of FE
  Stage 1 output contains:
  - problemStatement
  - proposedSolution
  - methodology
  - expectedContributions
end note

FE -> API: POST /v1/stages/queries\n{stage1Output}
activate API

API -> Auth: Verify JWT token
activate Auth
Auth -> Auth: Extract user ID
Auth --> API: User authenticated
deactivate Auth

API -> Controller: handleQueryGeneration(req)
activate Controller

Controller -> Controller: Validate input
note right
  - Check all fields present
  - Check fields not empty
end note

alt Invalid input
  Controller --> API: 400 Bad Request
  API --> FE: Error response
  FE --> User: Show validation error
else Valid input
  
  Controller -> Service: generateQueries(stage1Output, userId)
  activate Service
  
  Service -> Service: Prepare structured prompt
  note right
    Prompt engineering:
    "Based on this research intent, generate:
    1. 5-10 search query variations
    2. Key terms and keywords
    3. Boolean search queries
    4. Semantic search queries"
  end note
  
  Service -> OpenAI: POST /v1/chat/completions
  activate OpenAI
  
  alt API Success
    OpenAI --> Service: LLM response (JSON)
    deactivate OpenAI
    
    Service -> Service: Parse response
    Service -> Service: Optimize queries
    note right
      - Remove duplicates
      - Rank by relevance
      - Limit to top 10
    end note
    
    Service -> Service: Structure output
    note right
      {
        queries: [...],
        keywords: [...],
        booleanQueries: [...]
      }
    end note
    
    Service --> Controller: Query list
    deactivate Service
    
    Controller --> API: 200 OK + queries
    API --> FE: Success response
    FE --> User: Display generated queries
    
  else Rate Limit Exceeded
    OpenAI --> Service: 429 Rate Limit
    deactivate OpenAI
    Service --> Controller: Rate limit error
    deactivate Service
    Controller --> API: 429 Too Many Requests
    API --> FE: Error response
    FE --> User: "Too many requests, try again"
    
  else API Error
    OpenAI --> Service: 500 Error
    deactivate OpenAI
    Service -> Service: Log error
    Service --> Controller: API error
    deactivate Service
    Controller --> API: 500 Internal Server Error
    API --> FE: Error response
    FE --> User: "Service unavailable"
  end
  
end

deactivate Controller
deactivate API
deactivate FE

@enduml
